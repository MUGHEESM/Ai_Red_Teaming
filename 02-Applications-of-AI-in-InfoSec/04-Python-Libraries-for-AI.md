# Python Libraries for AI

Python is a versatile programming language widely used in Artificial Intelligence (AI) due to its rich library ecosystem that provides efficient and user-friendly tools for developing AI applications. This section focuses on two prominent Python libraries for AI development: Scikit-learn and PyTorch.

Just a quick note. This section provides a high-level overview of key Python libraries for AI, aiming to familiarize you with their purpose, structure, and common use cases. It offers a foundation for identifying relevant APIs and understanding the general landscape of these libraries. The official documentation will be your best resource to learning every small detail about the libraries. You do not need to copy and run these code snippets.

## Scikit-learn

Scikit-learn is a comprehensive library built on NumPy, SciPy, and Matplotlib. It offers a wide range of algorithms and tools for machine learning tasks and provides a consistent and intuitive API, making implementing various machine learning models easy.

- **Supervised Learning:** Scikit-learn provides a vast collection of supervised learning algorithms, including:
  - Linear Regression
  - Logistic Regression
  - Support Vector Machines (SVMs)
  - Decision Trees
  - Naive Bayes
  - Ensemble Methods (e.g., Random Forests, Gradient Boosting)
- **Unsupervised Learning:** It also offers various unsupervised learning algorithms, such as:
  - Clustering (K-Means, DBSCAN)
  - Dimensionality Reduction (PCA, t-SNE)
- **Model Selection and Evaluation:** Scikit-learn includes tools for model selection, hyperparameter tuning, and performance evaluation, enabling developers to optimize their models effectively.
- **Data Preprocessing:** It provides functionalities for data preprocessing, including:
  - Feature scaling and normalization
  - Handling missing values
  - Encoding categorical variables

### Data Preprocessing

Scikit-learn offers a rich set of tools for preprocessing data, a crucial step in preparing data for machine learning algorithms. These tools help transform raw data into a suitable format that improves the accuracy and efficiency of models.

Feature scaling is essential to ensure that all features have a similar scale, preventing features with larger values from dominating the learning process. Scikit-learn provides various scaling techniques:

- **StandardScaler:** Standardizes features by removing the mean and scaling to unit variance.
- **MinMaxScaler:** Scales features to a given range, typically between 0 and 1.
- **RobustScaler:** Scales features using statistics that are robust to outliers.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

Categorical features, representing data in categories or groups, need to be converted into numerical representations for machine learning algorithms to process them. Scikit-learn offers encoding techniques:

- **OneHotEncoder:** Creates binary (0 or 1) columns for each category.
- **LabelEncoder:** Assigns a unique integer to each category.

```python
from sklearn.preprocessing import OneHotEncoder

encoder = OneHotEncoder()
X_encoded = encoder.fit_transform(X)
```

Real-world datasets often contain missing values. Scikit-learn provides methods to handle these missing values:

- **SimpleImputer:** Replaces missing values with a specified strategy (e.g., mean, median, most frequent).
- **KNNImputer:** Imputes missing values using the k-Nearest Neighbors algorithm.

```python
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
```

### Model Selection and Evaluation

Scikit-learn offers tools for selecting the best model and evaluating its performance.

Splitting data into training and testing sets is crucial to evaluating the model's generalization ability to unseen data.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```

Cross-validation provides a more robust evaluation by splitting the data into multiple folds and training/testing on different combinations.

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
```

Scikit-learn provides various metrics to evaluate model performance:

- **accuracy_score:** For classification tasks.
- **mean_squared_error:** For regression tasks.
- **precision_score, recall_score, f1_score:** For classification tasks with imbalanced classes.

```python
from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)
```

### Model Training and Prediction

Scikit-learn follows a consistent API for training and predicting with different models.

Create an instance of the desired model with appropriate hyperparameters.

```python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(C=1.0)
```

Train the model using the fit() method with the training data.

```python
model.fit(X_train, y_train)
```

Make predictions on new data using the predict() method.

```python
y_pred = model.predict(X_test)
```

## PyTorch

PyTorch is an open-source machine learning library developed by Facebook's AI Research lab. It provides a flexible and powerful framework for building and deploying various types of machine learning models, including deep learning models.

### Key Features

- **Deep Learning:** PyTorch excels in deep learning, enabling the development of complex neural networks with multiple layers and architectures.
- **Dynamic Computational Graphs:** Unlike static computational graphs used in libraries like TensorFlow, PyTorch uses dynamic computational graphs, which allow for more flexible and intuitive model building and debugging.
- **GPU Support:** PyTorch supports GPU acceleration, significantly speeding up the training process for computationally intensive models.
- **TorchVision Integration:** TorchVision is a library integrated with PyTorch that provides a user-friendly interface for image datasets, pre-trained models, and common image transformations.
- **Automatic Differentiation:** PyTorch uses autograd to automatically compute gradients, simplifying the process of backpropagation.
- **Community and Ecosystem:** PyTorch has a large and active community, leading to a rich ecosystem of tools, libraries, and resources.

### Dynamic Computational Graphs and Tensors

At the heart of PyTorch lies the concept of dynamic computational graphs. A dynamic computational graph is created on the fly during the forward pass, allowing for more flexible and dynamic model building. This makes it easier to implement complex and non-linear models.

Tensors are multi-dimensional arrays that hold the data being processed. They can be constants, variables, or placeholders. PyTorch tensors are similar to NumPy arrays but can run on GPUs for faster computation.

```python
import torch

# Creating a tensor
x = torch.tensor([1.0, 2.0, 3.0])

# Tensors can be moved to GPU if available
if torch.cuda.is_available():
    x = x.to('cuda')
```

### Building Models with PyTorch

PyTorch provides a flexible and intuitive interface for building and training deep learning models. The torch.nn module contains various layers and modules for constructing neural networks.

The Sequential API allows building models layer by layer, adding each layer sequentially.

```python
import torch.nn as nn

model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10),
    nn.Softmax(dim=1)
)
```

The Module class provides more flexibility for building complex models with non-linear topologies, shared layers, and multiple inputs/outputs.

```python
import torch.nn as nn

class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.layer1 = nn.Linear(784, 128)
        self.relu = nn.ReLU()
        self.layer2 = nn.Linear(128, 10)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.layer2(x)
        x = self.softmax(x)
        return x

model = CustomModel()
```

### Training and Evaluation

PyTorch provides tools for training and evaluating models.

Optimizers are algorithms that adjust the model's parameters during training to minimize the loss function. PyTorch offers various optimizers:

- Adam
- SGD (Stochastic Gradient Descent)
- RMSprop

```python
import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=0.001)
```

Loss Functions measure the difference between the model's predictions and the actual target values. PyTorch provides a variety of loss functions:

- **CrossEntropyLoss:** For multi-class classification.
- **BCEWithLogitsLoss:** For binary classification.
- **MSELoss:** For regression.

```python
import torch.nn as nn

loss_fn = nn.CrossEntropyLoss()
```

Metrics evaluate the model's performance during training and testing.

- Accuracy
- Precision
- Recall

```python
def accuracy(output, target):
    _, predicted = torch.max(output, 1)
    correct = (predicted == target).sum().item()
    return correct / len(target)
```

The training loop updates the model's parameters based on the training data.

```python
import torch

epochs = 10
num_batches = 100

for epoch in range(epochs):
    for batch in range(num_batches):
        # Get batch of data
        x_batch, y_batch = get_batch(batch)
        
        # Forward pass
        y_pred = model(x_batch)
        
        # Calculate loss
        loss = loss_fn(y_pred, y_batch)
        
        # Backward pass and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Optional: print loss or other metrics
        if batch % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch+1}/{num_batches}], Loss: {loss.item():.4f}')
```

### Data Loading and Preprocessing

PyTorch provides the torch.utils.data.Dataset and DataLoader classes for handling data loading and preprocessing.

```python
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]

# Example usage
dataset = CustomDataset(data, labels)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

### Model Saving and Loading

PyTorch allows models to be saved and loaded for inference or further training.

```python
# Save model
torch.save(model.state_dict(), 'model.pth')

# Load model
model = CustomModel()
model.load_state_dict(torch.load('model.pth'))
model.eval()  # Set the model to evaluation mode
```
